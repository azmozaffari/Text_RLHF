{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0365b187",
   "metadata": {},
   "source": [
    "# Fine-Tune FLAN-T5 with Reinforcement Learning (PPO) and PEFT to Generate Less-Toxic Summaries\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd70a134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pip in /home/azadeh/.local/lib/python3.8/site-packages (23.2.1)\n",
      "\u001b[33mDEPRECATION: distro-info 0.23ubuntu1 has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of distro-info or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: python-debian 0.1.36ubuntu1 has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of python-debian or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mDEPRECATION: distro-info 0.23ubuntu1 has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of distro-info or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: python-debian 0.1.36ubuntu1 has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of python-debian or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu117\n",
      "Requirement already satisfied: torch==1.13.1+cu117 in /home/azadeh/.local/lib/python3.8/site-packages (1.13.1+cu117)\n",
      "Requirement already satisfied: torchvision==0.14.1+cu117 in /home/azadeh/.local/lib/python3.8/site-packages (0.14.1+cu117)\n",
      "Requirement already satisfied: typing-extensions in /home/azadeh/.local/lib/python3.8/site-packages (from torch==1.13.1+cu117) (4.2.0)\n",
      "Requirement already satisfied: numpy in /home/azadeh/.local/lib/python3.8/site-packages (from torchvision==0.14.1+cu117) (1.22.3)\n",
      "Requirement already satisfied: requests in /home/azadeh/.local/lib/python3.8/site-packages (from torchvision==0.14.1+cu117) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/azadeh/.local/lib/python3.8/site-packages (from torchvision==0.14.1+cu117) (9.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/azadeh/.local/lib/python3.8/site-packages (from requests->torchvision==0.14.1+cu117) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->torchvision==0.14.1+cu117) (2.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/azadeh/.local/lib/python3.8/site-packages (from requests->torchvision==0.14.1+cu117) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->torchvision==0.14.1+cu117) (2019.11.28)\n",
      "\u001b[33mDEPRECATION: distro-info 0.23ubuntu1 has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of distro-info or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: python-debian 0.1.36ubuntu1 has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of python-debian or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mDEPRECATION: distro-info 0.23ubuntu1 has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of distro-info or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: python-debian 0.1.36ubuntu1 has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of python-debian or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting git+https://github.com/lvwerra/trl.git@25fa1bd\n",
      "  Cloning https://github.com/lvwerra/trl.git (to revision 25fa1bd) to /tmp/pip-req-build-cebj9z2u\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/lvwerra/trl.git /tmp/pip-req-build-cebj9z2u\n",
      "\u001b[33m  WARNING: Did not find branch or tag '25fa1bd', assuming revision or ref.\u001b[0m\u001b[33m\n",
      "\u001b[0m  Running command git checkout -q 25fa1bd\n",
      "  Resolved https://github.com/lvwerra/trl.git to commit 25fa1bd\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: accelerate in /home/azadeh/.local/lib/python3.8/site-packages (from trl==0.4.2.dev0) (0.22.0)\n",
      "Requirement already satisfied: datasets in /home/azadeh/.local/lib/python3.8/site-packages (from trl==0.4.2.dev0) (2.11.0)\n",
      "Requirement already satisfied: numpy>=1.18.2 in /home/azadeh/.local/lib/python3.8/site-packages (from trl==0.4.2.dev0) (1.22.3)\n",
      "Requirement already satisfied: torch>=1.4.0 in /home/azadeh/.local/lib/python3.8/site-packages (from trl==0.4.2.dev0) (1.13.1+cu117)\n",
      "Requirement already satisfied: transformers>=4.18.0 in /home/azadeh/.local/lib/python3.8/site-packages (from trl==0.4.2.dev0) (4.27.2)\n",
      "Requirement already satisfied: typing-extensions in /home/azadeh/.local/lib/python3.8/site-packages (from torch>=1.4.0->trl==0.4.2.dev0) (4.2.0)\n",
      "Requirement already satisfied: filelock in /home/azadeh/.local/lib/python3.8/site-packages (from transformers>=4.18.0->trl==0.4.2.dev0) (3.12.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /home/azadeh/.local/lib/python3.8/site-packages (from transformers>=4.18.0->trl==0.4.2.dev0) (0.16.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/azadeh/.local/lib/python3.8/site-packages (from transformers>=4.18.0->trl==0.4.2.dev0) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers>=4.18.0->trl==0.4.2.dev0) (5.3.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/azadeh/.local/lib/python3.8/site-packages (from transformers>=4.18.0->trl==0.4.2.dev0) (2022.4.24)\n",
      "Requirement already satisfied: requests in /home/azadeh/.local/lib/python3.8/site-packages (from transformers>=4.18.0->trl==0.4.2.dev0) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/azadeh/.local/lib/python3.8/site-packages (from transformers>=4.18.0->trl==0.4.2.dev0) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/azadeh/.local/lib/python3.8/site-packages (from transformers>=4.18.0->trl==0.4.2.dev0) (4.64.0)\n",
      "Requirement already satisfied: psutil in /home/azadeh/.local/lib/python3.8/site-packages (from accelerate->trl==0.4.2.dev0) (5.9.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/azadeh/.local/lib/python3.8/site-packages (from datasets->trl==0.4.2.dev0) (13.0.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /home/azadeh/.local/lib/python3.8/site-packages (from datasets->trl==0.4.2.dev0) (0.3.6)\n",
      "Requirement already satisfied: pandas in /home/azadeh/.local/lib/python3.8/site-packages (from datasets->trl==0.4.2.dev0) (1.4.2)\n",
      "Requirement already satisfied: xxhash in /home/azadeh/.local/lib/python3.8/site-packages (from datasets->trl==0.4.2.dev0) (3.3.0)\n",
      "Requirement already satisfied: multiprocess in /home/azadeh/.local/lib/python3.8/site-packages (from datasets->trl==0.4.2.dev0) (0.70.14)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /home/azadeh/.local/lib/python3.8/site-packages (from datasets->trl==0.4.2.dev0) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /home/azadeh/.local/lib/python3.8/site-packages (from datasets->trl==0.4.2.dev0) (3.8.5)\n",
      "Requirement already satisfied: responses<0.19 in /home/azadeh/.local/lib/python3.8/site-packages (from datasets->trl==0.4.2.dev0) (0.18.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/azadeh/.local/lib/python3.8/site-packages (from aiohttp->datasets->trl==0.4.2.dev0) (21.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/azadeh/.local/lib/python3.8/site-packages (from aiohttp->datasets->trl==0.4.2.dev0) (3.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/azadeh/.local/lib/python3.8/site-packages (from aiohttp->datasets->trl==0.4.2.dev0) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/azadeh/.local/lib/python3.8/site-packages (from aiohttp->datasets->trl==0.4.2.dev0) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/azadeh/.local/lib/python3.8/site-packages (from aiohttp->datasets->trl==0.4.2.dev0) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/azadeh/.local/lib/python3.8/site-packages (from aiohttp->datasets->trl==0.4.2.dev0) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/azadeh/.local/lib/python3.8/site-packages (from aiohttp->datasets->trl==0.4.2.dev0) (1.3.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers>=4.18.0->trl==0.4.2.dev0) (2.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/azadeh/.local/lib/python3.8/site-packages (from requests->transformers>=4.18.0->trl==0.4.2.dev0) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers>=4.18.0->trl==0.4.2.dev0) (2019.11.28)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/azadeh/.local/lib/python3.8/site-packages (from pandas->datasets->trl==0.4.2.dev0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/azadeh/.local/lib/python3.8/site-packages (from pandas->datasets->trl==0.4.2.dev0) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas->datasets->trl==0.4.2.dev0) (1.14.0)\n",
      "\u001b[33mDEPRECATION: distro-info 0.23ubuntu1 has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of distro-info or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: python-debian 0.1.36ubuntu1 has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of python-debian or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install --disable-pip-version-check \\\n",
    "    torch \\\n",
    "    torchdata --quiet\n",
    "%pip install \\\n",
    "    torch==1.13.1+cu117 torchvision==0.14.1+cu117\\\n",
    "    --extra-index-url\\\n",
    "    https://download.pytorch.org/whl/cu117\n",
    "\n",
    "%pip install \\\n",
    "    transformers \\\n",
    "    datasets \\\n",
    "    evaluate \\\n",
    "    rouge_score \\\n",
    "    peft --quiet\n",
    "\n",
    "# Installing the Reinforcement Learning library directly from github.\n",
    "%pip install git+https://github.com/lvwerra/trl.git@25fa1bd    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d544198",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification, AutoModelForSeq2SeqLM, GenerationConfig,  TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "from peft import PeftModel, PeftConfig, LoraConfig, TaskType\n",
    "\n",
    "# trl: Transformer Reinforcement Learning library\n",
    "from trl import PPOTrainer, PPOConfig, AutoModelForSeq2SeqLMWithValueHead\n",
    "from trl import create_reference_model\n",
    "from trl.core import LengthSampler\n",
    "\n",
    "import torch\n",
    "import evaluate\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# tqdm library makes the loops show a smart progress meter.\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa94098e",
   "metadata": {},
   "source": [
    "### First we need the fine-tuned model to summarize the text.\n",
    "#### I use LoRA to fine-tune the Flan-T5 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "953b30c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/azadeh/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-931380d0e19583fc/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "553d1b13236a45659a9aeb8f6615860a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 12460\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dialogue and summary dataset\n",
    "model_name=\"google/flan-t5-base\"\n",
    "huggingface_dataset_name = \"knkarthick/dialogsum\"\n",
    "\n",
    "dataset_original = load_dataset(huggingface_dataset_name)\n",
    "\n",
    "dataset_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df8f1772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained model and its tokenizer\n",
    "model_name=\"google/flan-t5-base\"\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3304ef39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/azadeh/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-931380d0e19583fc/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-0760e73a230b4eed.arrow\n",
      "Loading cached processed dataset at /home/azadeh/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-931380d0e19583fc/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-90349f0fee0bae0d.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(example):\n",
    "    prompt = []\n",
    "    summ = []\n",
    "    \n",
    "    for idx, dialogue in enumerate(example[\"dialogue\"]):\n",
    "        prompt.append(f\"\"\"Summarize the following conversation.\n",
    "\n",
    "        {dialogue}\n",
    "\n",
    "        Summary:\n",
    "        \"\"\")\n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "        \n",
    "    example = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "    \n",
    "    \n",
    "#     i = torch.tensor(item).to('cuda:0')\n",
    "#         f.append(i)\n",
    "\n",
    "\n",
    "    return example\n",
    "\n",
    "\n",
    "\n",
    "tlr_datasets = dataset_original.map(tokenize_function, batched=True)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66abce70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/azadeh/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-931380d0e19583fc/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-d194f84673c2fb74.arrow\n",
      "Loading cached processed dataset at /home/azadeh/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-931380d0e19583fc/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-eb8feb074be70ebe.arrow\n",
      "Loading cached processed dataset at /home/azadeh/.cache/huggingface/datasets/knkarthick___csv/knkarthick--dialogsum-931380d0e19583fc/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1/cache-7f8d536b7a155430.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'labels', 'query'],\n",
      "        num_rows: 12460\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'labels', 'query'],\n",
      "        num_rows: 1500\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'labels', 'query'],\n",
      "        num_rows: 500\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# define the map function for the tokenizer to modify the prompt\n",
    "\n",
    "def tokenize_function(example):\n",
    "    prompt = []\n",
    "    summ = []\n",
    "    \n",
    "    \n",
    "    for idx, dialogue in enumerate(example[\"dialogue\"]):\n",
    "        prompt.append(f\"\"\"Summarize the following conversation.\n",
    "\n",
    "        {dialogue}\n",
    "\n",
    "        Summary:\n",
    "        \"\"\")\n",
    "        summ.append(example['summary'][idx])\n",
    "        \n",
    "#         prompt.append(torch.tensor(tokenizer(p, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids))\n",
    "\n",
    "\n",
    "#     print(prompt)  \n",
    "    example['input_ids'] = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    \n",
    "    example['labels'] = tokenizer(summ, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "#     example['attention_mask'] = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").attention_mask\n",
    "    example['query'] =  prompt\n",
    "    return example\n",
    "\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset_original.map(tokenize_function, batched=True)\n",
    "        \n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['id', 'topic', 'dialogue', 'summary',])\n",
    "print(tokenized_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e4471cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "lora_config = LoraConfig(\n",
    "    r=32, # Rank\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM # FLAN-T5\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "peft_model = get_peft_model(original_model, \n",
    "                            lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf2296bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "output_dir ='./model'\n",
    "\n",
    "peft_training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    auto_find_batch_size=True,\n",
    "    learning_rate=1e-3, # Higher learning rate than full fine-tuning.\n",
    "    num_train_epochs=1,\n",
    "    \n",
    ")\n",
    "    \n",
    "peft_trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=peft_training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53c6a916",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azadeh/.local/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33maz-mozaffari\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.11 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/azadeh/Documents/LLMs/lab/wandb/run-20230925_205644-h514us3x</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/az-mozaffari/huggingface/runs/h514us3x' target=\"_blank\">likely-firebrand-24</a></strong> to <a href='https://wandb.ai/az-mozaffari/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/az-mozaffari/huggingface' target=\"_blank\">https://wandb.ai/az-mozaffari/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/az-mozaffari/huggingface/runs/h514us3x' target=\"_blank\">https://wandb.ai/az-mozaffari/huggingface/runs/h514us3x</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1558' max='1558' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1558/1558 07:55, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.141800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.122000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.115700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1558, training_loss=0.4471316453887195, metrics={'train_runtime': 478.9049, 'train_samples_per_second': 26.018, 'train_steps_per_second': 3.253, 'total_flos': 8667537195663360.0, 'train_loss': 0.4471316453887195, 'epoch': 1.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d80fd5a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./model/tokenizer_config.json',\n",
       " './model/special_tokens_map.json',\n",
       " './model/tokenizer.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load fine tuned model from the memory\n",
    "peft_model_path=\"./model\"\n",
    "\n",
    "peft_trainer.model.save_pretrained(peft_model_path)\n",
    "tokenizer.save_pretrained(peft_model_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d5a1791",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"google/flan-t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "peft_model_path=\"./model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(peft_model_path)\n",
    "\n",
    "# generate some queries to test the accuracy of the new model and compare it to the original model\n",
    "dialogues = dataset_original['test'][0:20]['dialogue']\n",
    "human_baseline_summaries = dataset_original['test'][0:20]['summary']\n",
    "\n",
    "original_model_summaries = []\n",
    "instruct_model_summaries = []\n",
    "peft_model_summaries = []\n",
    "# original_model = original_model.to('cpu')\n",
    "\n",
    "for idx, dialogue in enumerate(dialogues):\n",
    "    prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary: \"\"\"\n",
    "    \n",
    "#     input_ids = tokenizer_original(prompt, return_tensors=\"pt\").input_ids\n",
    "#     input_ids = input_ids.to('cuda')\n",
    "    input_ids  = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    input_ids = input_ids.to('cuda:0')\n",
    "    human_baseline_text_output = human_baseline_summaries[idx]\n",
    "    \n",
    "    original_model_outputs = original_model.generate( input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "    input_ids = input_ids.to('cuda:0')\n",
    "    \n",
    "    peft_model_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200))\n",
    "    peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    original_model_summaries.append(original_model_text_output)\n",
    "    peft_model_summaries.append(peft_model_text_output)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c454f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL MODEL:\n",
      "{'rouge1': 0.3625816328737917, 'rouge2': 0.10143738651442691, 'rougeL': 0.2867149055312152, 'rougeLsum': 0.2871078425652874}\n",
      "PEFT MODEL:\n",
      "{'rouge1': 0.3916678434489629, 'rouge2': 0.1328566293652899, 'rougeL': 0.3052362517745412, 'rougeLsum': 0.3052010416808383}\n"
     ]
    }
   ],
   "source": [
    "# evaluate the accuracy of the trained peft model and compare to original model using rouge metric \n",
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "original_model_results = rouge.compute(\n",
    "    predictions=original_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(original_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "peft_model_results = rouge.compute(\n",
    "    predictions=peft_model_summaries,\n",
    "    references=human_baseline_summaries[0:len(peft_model_summaries)],\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "\n",
    "print('ORIGINAL MODEL:')\n",
    "print(original_model_results)\n",
    "print('PEFT MODEL:')\n",
    "print(peft_model_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71dc978",
   "metadata": {},
   "source": [
    "###  Load FLAN-T5 Fine Tuned Model, Prepare Reward Model and Toxicity Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81faaa19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the peft model from memory\n",
    "\n",
    "\n",
    "peft_model_base = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\", torch_dtype=torch.bfloat16)\n",
    "tokenizer_original = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(peft_model_base, \n",
    "                                       './model', \n",
    "                                       torch_dtype=torch.bfloat16,\n",
    "                                       is_trainable=False)\n",
    "tokenizer_peft = AutoTokenizer.from_pretrained(\"./model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abc95252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define lora model for training with rl \n",
    "lora_config = LoraConfig(\n",
    "    r=32, # Rank\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM # FLAN-T5\n",
    ")\n",
    "\n",
    "model_name = \"google/flan-t5-base\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, \n",
    "                                              torch_dtype=torch.bfloat16,\n",
    "                                             device_map=\"auto\")\n",
    "\n",
    "\n",
    "# lora model should be trainable \n",
    "peft_model = PeftModel.from_pretrained(model, \n",
    "                                       './model', \n",
    "                                       lora_config=lora_config,\n",
    "                                       torch_dtype=torch.bfloat16, \n",
    "                                       device_map=\"auto\",                                       \n",
    "                                       is_trainable=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d43b8712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of trainable parameters\n",
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"\\ntrainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20b383c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEFT model parameters to be updated:\n",
      "\n",
      "trainable model parameters: 3538944\n",
      "all model parameters: 251116800\n",
      "percentage of trainable model parameters: 1.41%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check the number of trainable parameters of base model\n",
    "print(f'PEFT model parameters to be updated:\\n{print_number_of_trainable_model_parameters(peft_model)}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "890506f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPO model parameters to be updated (ValueHead + 769 params):\n",
      "\n",
      "trainable model parameters: 3539713\n",
      "all model parameters: 251117569\n",
      "percentage of trainable model parameters: 1.41%\n",
      "\n",
      "ValueHead(\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (summary): Linear(in_features=768, out_features=1, bias=True)\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#define the training model with one layer Value head more for RL training  \n",
    "# peft_model = peft_model.to('cpu')\n",
    "ppo_model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(peft_model,                                                               \n",
    "                                                               torch_dtype=torch.bfloat16,\n",
    "                                                               device_map=\"auto\",\n",
    "                                                               is_trainable=True)\n",
    "\n",
    "print(f'PPO model parameters to be updated (ValueHead + 769 params):\\n{print_number_of_trainable_model_parameters(ppo_model)}\\n')\n",
    "print(ppo_model.v_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a4209ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference model parameters to be updated:\n",
      "\n",
      "trainable model parameters: 0\n",
      "all model parameters: 251117569\n",
      "percentage of trainable model parameters: 0.00%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# creat a refrence model with value head for KL divergence\n",
    "ref_model = create_reference_model(ppo_model)\n",
    "\n",
    "print(f'Reference model parameters to be updated:\\n{print_number_of_trainable_model_parameters(ref_model)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88300bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'nothate', 1: 'hate'}\n"
     ]
    }
   ],
   "source": [
    "### reward model for the toxicity \n",
    "# the logit layer score would be kept for the not-hate output as the reward\n",
    "toxicity_model_name = \"facebook/roberta-hate-speech-dynabench-r4-target\"\n",
    "toxicity_tokenizer = AutoTokenizer.from_pretrained(toxicity_model_name, device_map=\"auto\")\n",
    "toxicity_model = AutoModelForSequenceClassification.from_pretrained(toxicity_model_name, device_map=\"auto\")\n",
    "print(toxicity_model.config.id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9431e20b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits [not hate, hate]: [3.114102840423584, -2.489619255065918]\n",
      "probabilities [not hate, hate]: [0.9963293671607971, 0.003670602338388562]\n",
      "reward (high): [3.114102840423584]\n"
     ]
    }
   ],
   "source": [
    "# test a sample \n",
    "non_toxic_text = \"#Person 1# tells Tommy that he didn't like the movie.\"\n",
    "\n",
    "toxicity_input_ids = toxicity_tokenizer(non_toxic_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "toxicity_input_ids  = toxicity_input_ids.to('cuda:0')\n",
    "\n",
    "logits = toxicity_model(input_ids=toxicity_input_ids).logits\n",
    "print(f'logits [not hate, hate]: {logits.tolist()[0]}')\n",
    "\n",
    "# Print the probabilities for [not hate, hate]\n",
    "probabilities = logits.softmax(dim=-1).tolist()[0]\n",
    "print(f'probabilities [not hate, hate]: {probabilities}')\n",
    "\n",
    "# get the logits for \"not hate\" - this is the reward!\n",
    "not_hate_index = 0\n",
    "nothate_reward = (logits[:, not_hate_index]).tolist()\n",
    "print(f'reward (high): {nothate_reward}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3518b8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward model output:\n",
      "For non-toxic text\n",
      "[{'label': 'nothate', 'score': 3.114102840423584}, {'label': 'hate', 'score': -2.489619255065918}]\n",
      "[{'label': 'nothate', 'score': 0.9963293671607971}, {'label': 'hate', 'score': 0.003670602571219206}]\n"
     ]
    }
   ],
   "source": [
    "# define reward pipeline for rl model \n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "sentiment_pipe = pipeline(\"sentiment-analysis\", \n",
    "                          model=toxicity_model_name, \n",
    "                          device=device)\n",
    "reward_logits_kwargs = {\n",
    "    \"top_k\": None, # Return all scores.\n",
    "    \"function_to_apply\": \"none\", # Set to \"none\" to retrieve raw logits.\n",
    "    \"batch_size\": 16\n",
    "}\n",
    "\n",
    "reward_probabilities_kwargs = {\n",
    "    \"top_k\": None, # Return all scores.\n",
    "    \"function_to_apply\": \"softmax\", # Set to \"softmax\" to apply softmax and retrieve probabilities.\n",
    "    \"batch_size\": 16\n",
    "}\n",
    "\n",
    "print(\"Reward model output:\")\n",
    "print(\"For non-toxic text\")\n",
    "print(sentiment_pipe(non_toxic_text, **reward_logits_kwargs))\n",
    "print(sentiment_pipe(non_toxic_text, **reward_probabilities_kwargs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3c2ce8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "learning_rate=1.41e-5\n",
    "max_ppo_epochs=1\n",
    "mini_batch_size=4\n",
    "batch_size=512\n",
    "model_name=\"google/flan-t5-base\"\n",
    "# device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "device = 'cuda:0'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, \n",
    "                                              torch_dtype=torch.bfloat16,\n",
    "                                             device_map=\"auto\")\n",
    "# lora model should be trainable \n",
    "peft_model = PeftModel.from_pretrained(model, \n",
    "                                       './model', \n",
    "                                       lora_config=lora_config,\n",
    "                                       torch_dtype=torch.bfloat16, \n",
    "                                       device_map=\"auto\",                                       \n",
    "                                       is_trainable=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# peft_model = peft_model.to(device)\n",
    "ppo_model = AutoModelForSeq2SeqLMWithValueHead.from_pretrained(peft_model,                                                               \n",
    "                                                               torch_dtype=torch.bfloat16,\n",
    "                                                               device_map=\"auto\",\n",
    "                                                               is_trainable=True)\n",
    "ref_model = create_reference_model(ppo_model)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "config = PPOConfig(\n",
    "    model_name=model_name,    \n",
    "    learning_rate=learning_rate,\n",
    "    ppo_epochs=max_ppo_epochs,\n",
    "    mini_batch_size=mini_batch_size,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "def collator(data):\n",
    "    return dict((key, [d[key] for d in data]) for key in data[0])\n",
    "\n",
    "\n",
    "tokenizer_peft = AutoTokenizer.from_pretrained('./model',return_tensors=\"pt\")\n",
    "\n",
    "ppo_trainer = PPOTrainer(config=config, \n",
    "                         model=ppo_model, \n",
    "                         ref_model=ref_model, \n",
    "                         tokenizer=tokenizer_peft, \n",
    "                         dataset=tokenized_datasets[\"train\"],\n",
    "                         data_collator=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5e7b8302",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/home/azadeh/.local/lib/python3.8/site-packages/transformers/generation/utils.py:1288: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "1it [03:00, 180.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 17.10700798034668\n",
      "ppo/returns/mean: 0.6143479347229004\n",
      "ppo/policy/advantages_mean: -8.030651699186819e-10\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "2it [05:55, 177.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 15.933660507202148\n",
      "ppo/returns/mean: 0.7839390635490417\n",
      "ppo/policy/advantages_mean: -5.490540022634605e-09\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "3it [08:52, 177.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 15.4185791015625\n",
      "ppo/returns/mean: 0.8828891515731812\n",
      "ppo/policy/advantages_mean: 7.138047930510538e-10\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "4it [11:49, 177.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 16.317434310913086\n",
      "ppo/returns/mean: 0.7959517240524292\n",
      "ppo/policy/advantages_mean: 2.0143748891143787e-09\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "5it [14:46, 177.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 15.308398246765137\n",
      "ppo/returns/mean: 0.8806829452514648\n",
      "ppo/policy/advantages_mean: 4.5860595876412447e-10\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "6it [17:44, 177.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 15.318288803100586\n",
      "ppo/returns/mean: 0.9563939571380615\n",
      "ppo/policy/advantages_mean: -1.2055442200065158e-09\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "7it [20:40, 177.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 15.013477325439453\n",
      "ppo/returns/mean: 0.9635177850723267\n",
      "ppo/policy/advantages_mean: -3.211149346427078e-09\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "8it [23:42, 178.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 13.681775093078613\n",
      "ppo/returns/mean: 1.1279995441436768\n",
      "ppo/policy/advantages_mean: -2.384359598650576e-09\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azadeh/.local/lib/python3.8/site-packages/transformers/pipelines/base.py:1070: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "9it [26:46, 180.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 14.386486053466797\n",
      "ppo/returns/mean: 1.0782793760299683\n",
      "ppo/policy/advantages_mean: -2.737688742371347e-09\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "10it [29:43, 179.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objective/kl: 13.230035781860352\n",
      "ppo/returns/mean: 1.2634953260421753\n",
      "ppo/policy/advantages_mean: -4.4154462308654274e-09\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [29:44, 178.41s/it]\n"
     ]
    }
   ],
   "source": [
    "generation_kwargs = {\n",
    "    \"min_length\": 5,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True\n",
    "}\n",
    "\n",
    "reward_kwargs = {\n",
    "    \"top_k\": None, # Return all scores.\n",
    "    \"function_to_apply\": \"none\", # You want the raw logits without softmax.\n",
    "    \"batch_size\": 512\n",
    "}\n",
    "\n",
    "max_ppo_steps = 10\n",
    "max_len = 200\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for step, batch in tqdm(enumerate(ppo_trainer.dataloader)):\n",
    "    \n",
    "    if step >= max_ppo_steps:\n",
    "        break\n",
    "        \n",
    "       \n",
    "        \n",
    "        \n",
    "        \n",
    "    s = []\n",
    "    f = []\n",
    "    for item in batch[\"input_ids\"]: \n",
    "        i = torch.tensor(item).to('cuda:0')\n",
    "        f.append(i)\n",
    "        summary = ppo_trainer.generate(i,**generation_kwargs)\n",
    "        s.append(summary[0][0:max_len])\n",
    "    \n",
    "    \n",
    "    batch[\"response\"] = [tokenizer.decode(r) for r in s]\n",
    "   \n",
    "    \n",
    "    rewards = sentiment_pipe(batch[\"response\"], **reward_kwargs)\n",
    "\n",
    "    reward_tensors = [torch.tensor(reward[not_hate_index][\"score\"]) for reward in rewards]    \n",
    "\n",
    "    stats = ppo_trainer.step(f, s, reward_tensors)\n",
    "    ppo_trainer.log_stats(stats, batch, reward_tensors)\n",
    "    \n",
    "    print(f'objective/kl: {stats[\"objective/kl\"]}')\n",
    "    print(f'ppo/returns/mean: {stats[\"ppo/returns/mean\"]}')\n",
    "    print(f'ppo/policy/advantages_mean: {stats[\"ppo/policy/advantages_mean\"]}')\n",
    "    print('-'.join('' for x in range(100)))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1057963a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1e3bc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
